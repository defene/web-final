{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System Demo\n",
        "\n",
        "This notebook demonstrates the RAG system for Enhanced Groundedness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "from src.retrievers.sparse import BM25Retriever\n",
        "from src.retrievers.dense import DenseRetriever\n",
        "from src.generators.llm_generator import LLMGenerator\n",
        "from src.pipelines.base_rag import BaseRAGPipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Quick Start with Sample Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents\n",
        "documents = [\n",
        "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\",\n",
        "    \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials. It was built along the historical northern borders of China.\",\n",
        "    \"The Colosseum is an oval amphitheatre in the centre of the city of Rome, Italy. It is the largest ancient amphitheatre ever built.\",\n",
        "    \"Machu Picchu is a 15th-century Inca citadel situated on a mountain ridge in Peru. It is the most familiar icon of Inca civilization.\",\n",
        "    \"The Taj Mahal is an ivory-white marble mausoleum on the right bank of the river Yamuna in Agra, India. It was commissioned in 1632 by the Mughal emperor Shah Jahan.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create BM25 retriever\n",
        "sparse_retriever = BM25Retriever(top_k=3)\n",
        "sparse_retriever.index(documents)\n",
        "\n",
        "# Test retrieval\n",
        "query = \"Where is the Eiffel Tower located?\"\n",
        "results = sparse_retriever.retrieve(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(\"\\nRetrieved documents:\")\n",
        "for doc in results:\n",
        "    print(f\"  Score: {doc.score:.4f}\")\n",
        "    print(f\"  Content: {doc.content[:100]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dense Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Dense retriever (uses BGE embeddings by default)\n",
        "dense_retriever = DenseRetriever(\n",
        "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
        "    top_k=3,\n",
        "    device=\"cuda\"  # Change to 'cpu' if no GPU\n",
        ")\n",
        "dense_retriever.index(documents)\n",
        "\n",
        "# Test retrieval\n",
        "results = dense_retriever.retrieve(query)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(\"\\nRetrieved documents (Dense):\")\n",
        "for doc in results:\n",
        "    print(f\"  Score: {doc.score:.4f}\")\n",
        "    print(f\"  Content: {doc.content[:100]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Full RAG Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create generator (OpenAI backend)\n",
        "# Make sure to set OPENAI_API_KEY environment variable\n",
        "generator = LLMGenerator(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    backend=\"openai\"\n",
        ")\n",
        "\n",
        "# Create RAG pipeline\n",
        "rag_pipeline = BaseRAGPipeline(\n",
        "    retriever=dense_retriever,\n",
        "    generator=generator,\n",
        "    top_k=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask a question\n",
        "question = \"Who designed the Eiffel Tower?\"\n",
        "result = rag_pipeline.query(question)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"\\nAnswer: {result.answer}\")\n",
        "print(f\"\\nLatency: {result.latency_ms:.2f} ms\")\n",
        "print(f\"\\nRetrieved {len(result.retrieved_documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Local Model (HuggingFace)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create generator with local model\n",
        "local_generator = LLMGenerator(\n",
        "    model_name=\"google/flan-t5-large\",\n",
        "    backend=\"huggingface\",\n",
        "    device=\"cuda\"\n",
        ")\n",
        "\n",
        "# Create RAG pipeline with local model\n",
        "local_rag = BaseRAGPipeline(\n",
        "    retriever=dense_retriever,\n",
        "    generator=local_generator,\n",
        "    top_k=3\n",
        ")\n",
        "\n",
        "result = local_rag.query(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {result.answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.evaluation.retrieval_metrics import RetrievalEvaluator, QAEvaluator\n",
        "\n",
        "# Sample evaluation data\n",
        "test_questions = [\n",
        "    \"Where is the Eiffel Tower?\",\n",
        "    \"What is the Colosseum?\",\n",
        "    \"Where is Machu Picchu?\"\n",
        "]\n",
        "ground_truths = [\n",
        "    \"Paris, France\",\n",
        "    \"An oval amphitheatre in Rome, Italy\",\n",
        "    \"Peru\"\n",
        "]\n",
        "\n",
        "# Run queries\n",
        "results = rag_pipeline.batch_query(test_questions)\n",
        "predictions = [r.answer for r in results]\n",
        "\n",
        "# Evaluate\n",
        "qa_evaluator = QAEvaluator()\n",
        "metrics = qa_evaluator.evaluate(predictions, ground_truths)\n",
        "\n",
        "print(\"QA Evaluation Results:\")\n",
        "print(f\"  Exact Match: {metrics['exact_match']:.4f}\")\n",
        "print(f\"  F1 Score: {metrics['f1']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
